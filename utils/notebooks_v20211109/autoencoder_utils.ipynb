{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "residential-essence",
   "metadata": {},
   "source": [
    "**Utilities related to the training and evaluation of autoencoder models with keras**\n",
    "\n",
    "The functionality in this script includes:\n",
    "- definition of loss functions (several flavours of MSE or chi-squared)\n",
    "- calculating and plotting ROC curves and confusion matrices\n",
    "- definition of very simple ready-to-use keras model architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "approved-bikini",
   "metadata": {},
   "outputs": [],
   "source": [
    "### imports\n",
    "\n",
    "# external modules\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import importlib\n",
    "\n",
    "# local modules\n",
    "import plot_utils\n",
    "importlib.reload(plot_utils)\n",
    "from notebook_utils.notebook_to_script import save_notebook_as_script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "criminal-sterling",
   "metadata": {},
   "outputs": [],
   "source": [
    "### define loss functions\n",
    "\n",
    "def mseTop10(y_true, y_pred):\n",
    "    ### MSE top 10 loss function for autoencoder training\n",
    "    # input arguments:\n",
    "    # - y_true and y_pred: two numpy arrays of equal shape,\n",
    "    #   typically a histogram and its autoencoder reconstruction.\n",
    "    #   if two-dimensional, the arrays are assumed to have shape (nhists,nbins)!\n",
    "    # output:\n",
    "    # - mean squared error between y_true and y_pred,\n",
    "    #   where only the 10 bins with largest squared error are taken into account.\n",
    "    #   if y_true and y_pred are 2D arrays, this function returns 1D array (mseTop10 for each histogram)\n",
    "    top_values, _ = tf.nn.top_k(K.square(y_pred - y_true), k=10, sorted=True)\n",
    "    mean=K.mean(top_values, axis=-1)\n",
    "    return mean\n",
    "\n",
    "def mseTop10Raw(y_true, y_pred):\n",
    "    ### same as mseTop10 but without using tf or K\n",
    "    # the version including tf or K seemed to cause randomly dying kernels, no clear reason could be found,\n",
    "    # but it was solved using this loss function instead.\n",
    "    # verified that it gives exactly the same output as the function above on some random arrays.\n",
    "    # contrary to mseTop10, this function only works for arrays with 2D shapes (so shape (nhists,nbins)), not for (nbins,).\n",
    "    sqdiff = np.power(y_true-y_pred,2)\n",
    "    sqdiff[:,::-1].sort()\n",
    "    sqdiff = sqdiff[:,:10]\n",
    "    mean = np.mean(sqdiff,axis=-1)\n",
    "    return mean\n",
    "\n",
    "def mseTopNRaw(y_true, y_pred, n=10):\n",
    "    ### generalization of mseTop10Raw to any number of bins to take into account\n",
    "    # note: now generalized to also work for 2D histograms, i.e. arrays of shape (nhists,nybins,nxbins)!\n",
    "    #       hence this is the most general method and preferred above mseTop10 and mseTop10Raw, which are only kept for reference\n",
    "    # input arguments:\n",
    "    # - y_true, y_pred: numpy arrays between which to calculate the mean square difference, of shape (nhists,nbins) or (nhists,nybins,nxbins)\n",
    "    # - n: number of largest elements to keep for averaging\n",
    "    # output:\n",
    "    # numpy array of shape (nhists)\n",
    "    sqdiff = np.power(y_true-y_pred,2)\n",
    "    if len(sqdiff.shape)==3:\n",
    "        sqdiff = sqdiff.reshape(len(sqdiff),-1)\n",
    "    sqdiff = np.partition( sqdiff, -n, axis=-1 )[:,-n:]\n",
    "    mean = np.mean( sqdiff, axis=-1 )\n",
    "    return mean\n",
    "\n",
    "# attempts to use chi2 instead of mse, so far no good results, but keep for reference\n",
    "def chiSquared(y_true, y_pred):\n",
    "    ### chi2 loss function for autoencoder training\n",
    "    # input arguments:\n",
    "    # - y_true and y_pred: two numpy arrays of equal shape,\n",
    "    #   typically a histogram and its autoencoder reconstruction.\n",
    "    #   if two-dimensional, the arrays are assumed to have shape (nhists,nbins)!\n",
    "    # output:\n",
    "    # - relative mean squared error between y_true and y_pred,\n",
    "    #   if y_true and y_pred are 2D arrays, this function returns 1D array (chiSquared for each histogram)\n",
    "    normdiffsq = np.divide(K.square(y_pred - y_true),y_true)\n",
    "    chi2 = K.sum(normdiffsq,axis=-1)\n",
    "    return chi2\n",
    "\n",
    "def chiSquaredTopNRaw(y_true, y_pred, n=10):\n",
    "    ### generalization of chiSquared to any number of bins to take into account\n",
    "    # note: should work for 2D histograms as well (i.e. arrays of shape (nhistograms,nybins,nxbins)),\n",
    "    #       but not yet tested!\n",
    "    # input arguments:\n",
    "    # - y_true, y_pred: numpy arrays between which to calculate the mean square difference, of shape (nhists,nbins) or (nhists,nybins,nxbins)\n",
    "    # - n: number of largest elements to keep for summing\n",
    "    # output:\n",
    "    # numpy array of shape (nhists)\n",
    "    sqdiff = np.power(y_true-y_pred,2)\n",
    "    chi2 = np.where(y_true==0,0,sqdiff/y_true)\n",
    "    if len(chi2.shape)==3:\n",
    "        chi2 = chi2.reshape(len(chi2),-1)\n",
    "    chi2 = np.partition( chi2, -n, axis=-1 )[:,-n:]\n",
    "    chi2 = np.sum( chi2, axis=-1 )\n",
    "    return chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "literary-jacket",
   "metadata": {},
   "outputs": [],
   "source": [
    "### get roc curve and auc score in case labels are known\n",
    "\n",
    "def calculate_roc(scores, labels, scoreax):\n",
    "    ### calculate a roc curve\n",
    "    # input arguments:\n",
    "    # - scores is a 1D numpy array containing output scores of any algorithm\n",
    "    # - labels is a 1D numpy array (equally long as scores) containing labels\n",
    "    #   note that 1 for signal and 0 for background is assumed!\n",
    "    #   this convention is only used to define what scores belong to signal or background;\n",
    "    #   the scores itself can be anything (not limited to (0,1)), \n",
    "    #   as long as the target for signal is higher than the target for background\n",
    "    # - scoreax is an array of score thresholds for which to compute the signal and background efficiency,\n",
    "    #   assumed to be sorted in increasing order (i.e. from loose to tight)\n",
    "    # output:\n",
    "    # - tuple of two np arrays (signal efficiency and background efficiency)\n",
    "    nsig = np.sum(labels)\n",
    "    nback = np.sum(1-labels)\n",
    "    sig_eff = np.zeros(len(scoreax))\n",
    "    bkg_eff = np.zeros(len(scoreax))\n",
    "    for i,scorethreshold in enumerate(scoreax):\n",
    "        sig_eff[i] = np.sum(np.where((labels==1) & (scores>scorethreshold),1,0))/nsig\n",
    "        bkg_eff[i] = np.sum(np.where((labels==0) & (scores>scorethreshold),1,0))/nback\n",
    "    return (sig_eff,bkg_eff)\n",
    "\n",
    "def get_roc(scores, labels, mode='lin', npoints=100, doprint=False, doplot=True, plotmode='classic', doshow=True):\n",
    "    ### make a ROC curve\n",
    "    # input arguments:\n",
    "    # - scores is a 1D numpy array containing output scores of any algorithm\n",
    "    # - labels is a 1D numpy array (equally long as scores) containing labels\n",
    "    #   note that 1 for signal and 0 for background is assumed!\n",
    "    #   this convention is only used to define what scores belong to signal or background;\n",
    "    #   the scores itself can be anything (not limited to (0,1)), \n",
    "    #   as long as the target for signal is higher than the target for background\n",
    "    # - mode: how to determine the points where to calculate signal and background efficiencies; options are:\n",
    "    #         - 'lin': np.linspace between min and max score\n",
    "    #         - 'geom': np. geomspace between min and max score\n",
    "    #         - 'full': one point per score instance\n",
    "    # - npoints: number of points where to calculate the signal and background efficiencies\n",
    "    #   (ignored if mode is 'full')\n",
    "    # - doprint: boolean whether to print score thresholds and corresponding signal and background efficiencies\n",
    "    # - doplot: boolean whether to make a plot or simply return the auc.\n",
    "    # - plotmode: how to plot the roc curve; options are:\n",
    "    #         - 'classic' = signal efficiency afo background efficiency\n",
    "    \n",
    "    mlist = ['lin','geom','full']\n",
    "    if not mode in mlist:\n",
    "        raise Exception('ERROR in autoencoder_utils.py / get_roc: mode {} not recognized;'.format(mode)\n",
    "                       +' options are: {}'.format(mlist))\n",
    "    \n",
    "    if mode=='full':\n",
    "        scoreax = np.sort(scores)\n",
    "        scoreax[-1] += 0.01 # make sure the extremal scores are fully covered\n",
    "        scoreax[0] -= 0.01 # make sure the extremal scores are fully covered\n",
    "    elif mode=='lin':\n",
    "        scoremin = np.amin(scores)-1e-7\n",
    "        scoremax = np.amax(scores)+1e-7\n",
    "        scoreax = np.linspace(scoremin,scoremax,num=npoints)\n",
    "    elif mode=='geom':\n",
    "        scoremin = np.amin(scores)-1e-7\n",
    "        # if minimum score is below zero, shift everything up (needed for geomspace)\n",
    "        if scoremin < 0.: \n",
    "            scores = scores - scoremin + 1.\n",
    "            scoremin = 1.\n",
    "        scoremax = np.amax(scores)+1e-7\n",
    "        scoreax = np.geomspace(scoremin,scoremax,num=npoints)\n",
    "    \n",
    "    (sig_eff,bkg_eff) = calculate_roc( scores, labels, scoreax )\n",
    "    if doprint:\n",
    "        print('calculating roc curve:')\n",
    "        for i in range(len(scoreax)):\n",
    "            #print('  threshold: {:.4e}, signal: {:.4f}, background: {:.4f}'.format(scoreax[i],sig_eff[i],bkg_eff[i]))\n",
    "            print('  threshold: {}, signal: {}, background: {}'.format(scoreax[i],sig_eff[i],bkg_eff[i]))\n",
    "    \n",
    "    # note: sig_eff = signal efficiency = tp = true positive = signal flagged as signal\n",
    "    # note: bkg_eff = background efficiency = fp = false positive = background flagged as signal\n",
    "    fn = 1 - sig_eff # signal marked as background\n",
    "    tn = 1 - bkg_eff # background marked as background\n",
    "    \n",
    "    \n",
    "    auc = np.trapz(sig_eff[::-1],bkg_eff[::-1])\n",
    "    \n",
    "    if not doplot:\n",
    "        return auc\n",
    "    \n",
    "    # calculate auc\n",
    "    if plotmode=='classic':\n",
    "        # make plot\n",
    "        fig,ax = plt.subplots()\n",
    "        ax.scatter(bkg_eff,sig_eff)\n",
    "        ax.set_title('ROC curve')\n",
    "        # general axis titles:\n",
    "        #ax.set_xlabel('background effiency (background marked as signal)')\n",
    "        #ax.set_ylabel('signal efficiency (signal marked as signal)')\n",
    "        # specific axis titles:\n",
    "        ax.set_xlabel('good histograms flagged as anomalous')\n",
    "        ax.set_ylabel('bad histograms flagged as anomalous')\n",
    "        ax.set_xscale('log')\n",
    "        # set x axis limits\n",
    "        ax.set_xlim((np.amin(np.where(bkg_eff>0.,bkg_eff,1.))/2.,1.))\n",
    "        # set y axis limits: general case from 0 to 1.\n",
    "        #ax.set_ylim(0.,1.1)\n",
    "        # set y axis limits: adaptive limits based on measured signal efficiency array.\n",
    "        ylowlim = np.amin(np.where((sig_eff>0.) & (bkg_eff>0.),sig_eff,1.))\n",
    "        ylowlim = 2*ylowlim-1.\n",
    "        ax.set_ylim((ylowlim,1+(1-ylowlim)/5))\n",
    "        ax.grid()\n",
    "        auctext = '{:.3f}'.format(auc)\n",
    "        if auc>0.99:\n",
    "            auctext = '1 - '+'{:.3e}'.format(1-auc)\n",
    "        ax.text(0.7,0.1,'AUC: '+auctext,transform=ax.transAxes)\n",
    "        if doshow: plt.show()\n",
    "        \n",
    "    else:\n",
    "        print('ERROR: mode not recognized: '+str(mode))\n",
    "        return 0\n",
    "    \n",
    "    return auc\n",
    "\n",
    "def get_roc_from_hists(hists, labels, predicted_hists, mode='lin', npoints=100, doprint=False, doplot=True, plotmode='classic'):\n",
    "    ### make a ROC curve without manually calculating the scores\n",
    "    # the output score is the mseTop10Raw between the histograms and their reconstruction\n",
    "    # - input arguments:\n",
    "    # - hists and predicted_hists are 2D numpy arrays of shape (nhistograms,nbins)\n",
    "    # - other arguments: see get_roc\n",
    "\n",
    "    # determine mse\n",
    "    mse = mseTop10Raw(hists, predicted_hists)\n",
    "    # score equals mse, since larger mse = more signal-like (signal=anomalies)\n",
    "    return get_roc(mse,labels,mode=mode,npoints=npoints,doprint=doprint,doplot=doplot,plotmode=plotmode)\n",
    "\n",
    "def get_confusion_matrix(scores, labels, wp=None):\n",
    "    ### plot a confusion matrix\n",
    "    # scores and labels are defined in the same way as for get_roc\n",
    "    # wp is the chosen working point \n",
    "    # (i.e. any score above wp is flagged as signal, any below is flagged as background)\n",
    "    \n",
    "    if wp is None:\n",
    "        raise Exception('ERROR in get_confusion_matrix: you must provide a working point with the keyword option wp=...')\n",
    "    \n",
    "    nsig = np.sum(labels)\n",
    "    nback = np.sum(1-labels)\n",
    "    \n",
    "    # get confusion matrix entries\n",
    "    tp = np.sum(np.where((labels==1) & (scores>wp),1,0))/nsig\n",
    "    fp = np.sum(np.where((labels==0) & (scores>wp),1,0))/nback\n",
    "    tn = 1-fp\n",
    "    fn = 1-tp\n",
    "    cmat = np.array([[tp,fn],[fp,tn]])\n",
    "    # general labels:\n",
    "    #df_cm = pd.DataFrame(cmat, index = ['signal','background'],\n",
    "    #              columns = ['predicted signal','predicted background'])\n",
    "    # specific labels:\n",
    "    df_cm = pd.DataFrame(cmat, index = ['bad','good'],\n",
    "                  columns = ['predicted anomalous','predicted good'])\n",
    "    #plt.figure(figsize = (10,7))\n",
    "    plt.figure()\n",
    "    sn.heatmap(df_cm, annot=True, cmap=plt.cm.Blues)\n",
    "    \n",
    "def get_confusion_matrix_from_hists(hists, labels, predicted_hists, msewp=None):\n",
    "    ### plot a confusion matrix without manually calculating the scores\n",
    "    # the output score is the mse between the histograms and their reconstruction\n",
    "    \n",
    "    # get mse\n",
    "    mse = mseTop10Raw(hists, predicted_hists)\n",
    "    get_confusion_matrix(mse, labels, wp=msewp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opposite-identification",
   "metadata": {},
   "outputs": [],
   "source": [
    "### getting a keras model ready for training with minimal user inputs\n",
    "\n",
    "def getautoencoder(input_size,arch,act=[],opt='adam',loss=mseTop10):\n",
    "    ### get a trainable autoencoder model\n",
    "    # input args:\n",
    "    # - input_size: size of vector that autoencoder will operate on\n",
    "    # - arch: list of number of nodes per hidden layer (excluding input and output layer)\n",
    "    # - act: list of activations per layer (default: tanh)\n",
    "    # - opt: optimizer to use (default: adam)\n",
    "    # - loss: loss function to use (defualt: mseTop10)\n",
    "    \n",
    "    import math\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "    from tensorflow.keras.layers import Input, Dense\n",
    "    from keras.layers.advanced_activations import PReLU\n",
    "    from tensorflow.keras.models import Model, Sequential, load_model\n",
    "    from keras import backend as K\n",
    "    \n",
    "    if len(act)==0: act = ['tanh']*len(arch)\n",
    "    layers = []\n",
    "    # first layer manually to set input_dim\n",
    "    layers.append(Dense(arch[0],activation=act[0],input_dim=input_size))\n",
    "    # rest of layers in a loop\n",
    "    for nnodes,activation in zip(arch[1:],act[1:]):\n",
    "        layers.append(Dense(nnodes,activation=activation))\n",
    "    # last layer is decoder\n",
    "    layers.append(Dense(input_size,activation='tanh'))\n",
    "    autoencoder = Sequential()\n",
    "    for i,l in enumerate(layers):\n",
    "        #l.name = 'layer_'+str(i)\n",
    "        autoencoder.add(l)\n",
    "    autoencoder.compile(optimizer=opt, loss=loss)\n",
    "    autoencoder.summary()\n",
    "    return autoencoder\n",
    "\n",
    "def train_simple_autoencoder(hists, nepochs=-1, modelname='', \n",
    "                             batch_size=500, shuffle=False, \n",
    "                             verbose=1, validation_split=0.1):\n",
    "    ### create and train a very simple keras model\n",
    "    # the model consists of one hidden layer (with half as many units as there are input bins), tanh activation, adam optimizer and mseTop10 loss.\n",
    "    # input args: \n",
    "    # - hists is a 2D numpy array of shape (nhistograms, nbins)\n",
    "    # - nepochs is the number of epochs to use (has a default value if left unspecified)\n",
    "    # - modelname is a file name to save the model in (default: model is not saved to a file)\n",
    "    input_size = hists.shape[1]\n",
    "    arch = [int(hists.shape[1]/2.)]\n",
    "    act = ['tanh']*len(arch)\n",
    "    opt = 'adam'\n",
    "    loss = mseTop10\n",
    "    if nepochs<0: nepochs = int(min(40,len(hists)/400))\n",
    "    model = getautoencoder(input_size,arch,act=act,opt=opt,loss=loss)\n",
    "    history = model.fit(hists, hists, epochs=nepochs, batch_size=batch_size, \n",
    "                        shuffle=shuffle, verbose=verbose, \n",
    "                        validation_split=validation_split)\n",
    "    plot_utils.plot_loss(history)\n",
    "    if len(modelname)>0: model.save(modelname.split('.')[0]+'.h5')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wooden-spouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "### replacing scores of +-inf with sensible value\n",
    "\n",
    "def clip_scores( scores ):\n",
    "    ### clip +-inf values in scores\n",
    "    # +inf values in scores will be replaced by the maximum value (exclucing +inf) plus one\n",
    "    # -inf values in scores will be replaced by the minimim value (exclucing -inf) minus one\n",
    "    # input arguments:\n",
    "    # - scores: 1D numpy array\n",
    "    # returns\n",
    "    # - array with same length as scores with elements replaced as explained above\n",
    "    maxnoninf = np.max(np.where(scores==np.inf,np.min(scores),scores)) + 1\n",
    "    minnoninf = np.min(np.where(scores==-np.inf,np.max(scores),scores)) -1\n",
    "    if np.max(scores)>maxnoninf: \n",
    "        scores = np.where(scores==np.inf,maxnoninf,scores)\n",
    "        print('NOTE: scores of +inf were reset to {}'.format(maxnoninf))\n",
    "    if np.min(scores)<minnoninf:\n",
    "        scores = np.where(scores==-np.inf,minnoninf,scores)\n",
    "        print('NOTE: scores of -inf were reset to {}'.format(minnoninf))\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empty-delta",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_notebook_as_script( 'autoencoder_utils.ipynb' )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
